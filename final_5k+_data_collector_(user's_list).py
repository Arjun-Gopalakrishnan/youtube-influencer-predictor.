# -*- coding: utf-8 -*-
"""Final 5k+ Data Collector (User's List)

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1T-u3SoSdC47REZUAnCynMTAbOTjMgd2j
"""

import pandas as pd
from googleapiclient.discovery import build
import time
import os

# --- CONFIGURATION ---
API_KEY = 'AIzaSyDpz-P9FFvqYJ_yky9o_MZkJAY61016klI'
CHANNEL_IDS = [
    'UClfll_TO6FpOPLUg4Aawhlw',    # 1. M4 Tech
    'UC9MQp8a5uhaIosZPHaoqEXQ',    # 2. CallMeShazzam
    'UCM9fQt4Fw4-GxjBwt_PC7bA',    # 3. Ratheesh R Menon
    'UCDEEw71KgDrxG61_p7U58ZA',    # 4. Jayaraj G Nath
    'UC5p2HB41JVstSICaaqOr_oA',    # 5. GADGETS ONE MALAYALAM TECH TIPS
    'UCCya-YUszMuiTAR3lfYMuog',    # 6. Mr. Perfect Tech
    'UCHhytYnSiFnB8ZUnayWncjw',    # 7. Rickyrodger
    'UCHLmaPy_UYFvQcUp-FdpxRg',    # 8. Sarath'S Neon Tech
    'UCLVECJWnapbCQ3FokW19cFQ',    # 9. ADOPIX
    'UCn4S6ONdetkpxfuOteHVHhQ',    # 10. PrathapGTech
    'UCBnnsrvmuQ7tFdTL511dzBQ',    # 11. Malayalam Tech - മലയാളം ടെക്
    'UCSL1rBX187wwNwyANAoZrBg',    # 12. Tec Tok by Hareesh
    'UCHepftuP68VmXFkAI8nIPNg',    # 13. Mallu Discovery TV
    'UCx9UjTrAXlt4UIMa_-GXVeQ',    # 14. Mallu Tech
    'UC5uIWd_FJNNeKj5bL_uJMKg',    # 15. MalayaliPada - Vlogs & Tech Reviews from USA
    'UCx55WMnUN-0DDiy-OdTqvWg',    # 16. Tech Frame Malayalam
    'UCJPYTDq_fsywsegZpdCjuxQ',    # 17. Tech 4 Malayalam
]


CSV_FILE_NAME = 'youtube_tech_data_final.csv'
# --------------------

def get_channel_stats(youtube, channel_id):
    try:
        request = youtube.channels().list(part='snippet,statistics', id=channel_id)
        response = request.execute()
        if not response.get('items'): return None, None
        item = response['items'][0]
        return item['snippet']['title'], int(item['statistics'].get('subscriberCount', 0))
    except Exception as e:
        print(f"    ERROR fetching channel stats for {channel_id}: {e}")
        return None, None

def get_video_ids(youtube, channel_id):
    try:
        res = youtube.channels().list(id=channel_id, part='contentDetails').execute()
        if not res.get('items'): return []
        playlist_id = res['items'][0]['contentDetails']['relatedPlaylists']['uploads']
        videos = []
        next_page_token = None
        while True:
            res = youtube.playlistItems().list(playlistId=playlist_id, part='contentDetails', maxResults=50, pageToken=next_page_token).execute()
            videos += [item['contentDetails']['videoId'] for item in res['items']]
            next_page_token = res.get('nextPageToken')
            if next_page_token is None: break
        return videos
    except Exception as e:
        print(f"    ERROR fetching video IDs for {channel_id}: {e}")
        return []

def get_video_details(youtube, video_ids):
    video_details = []
    try:
        for i in range(0, len(video_ids), 50):
            chunk = video_ids[i:i+50]
            res = youtube.videos().list(id=','.join(chunk), part='snippet,statistics').execute()
            for item in res['items']:
                stats = item.get('statistics', {})
                video_details.append({
                    'video_title': item['snippet']['title'],
                    'view_count': int(stats.get('viewCount', 0)),
                    'like_count': int(stats.get('likeCount', 0)),
                    'comment_count': int(stats.get('commentCount', 0))
                })
        return video_details
    except Exception as e:
        print(f"    ERROR fetching video details: {e}")
        return video_details

if __name__ == '__main__':
    youtube = build('youtube', 'v3', developerKey=API_KEY)

    collected_channels = set()
    if os.path.exists(CSV_FILE_NAME):
        print("Existing data file found. Resuming collection...")
        df_existing = pd.read_csv(CSV_FILE_NAME)
        collected_channels = set(df_existing['channel_name'])
        print(f"Already collected data for {len(collected_channels)} channels.")

    all_data = []
    total_channels = len(CHANNEL_IDS)

    print(f"--- Starting Final 5k+ Data Collection Run ({total_channels} Channels) ---")
    for i, channel_id in enumerate(CHANNEL_IDS):
        print(f"\nProcessing channel {i+1}/{total_channels} (ID: {channel_id})")

        channel_name, subscriber_count = get_channel_stats(youtube, channel_id)

        if not channel_name:
            print(f"--> Skipping: Could not fetch channel name. This is likely an API Quota error. Try again tomorrow.")
            continue

        if channel_name in collected_channels:
            print(f"--> Skipping '{channel_name}': Already collected.")
            continue

        video_ids = get_video_ids(youtube, channel_id)
        if not video_ids:
            print(f"--> Skipping '{channel_name}': No videos. (Channel may be new or empty)")
            continue

        video_details = get_video_details(youtube, video_ids=video_ids)

        for video in video_details:
            video['channel_name'] = channel_name
            video['subscriber_count'] = subscriber_count
            all_data.append(video)

        print(f"--> SUCCESS: Collected {len(video_details)} videos from '{channel_name}'.")
        time.sleep(0.5)

    if all_data:
        df_new = pd.DataFrame(all_data)

        if os.path.exists(CSV_FILE_NAME):
            df_new.to_csv(CSV_FILE_NAME, mode='a', header=False, index=False, encoding='utf-8')
        else:
            df_new.to_csv(CSV_FILE_NAME, index=False, encoding='utf-8')

        print(f"\n✨ Batch complete. Appended {len(df_new)} new video records.")
    else:
        print("\nNo new data collected in this run. This is likely due to API quota. Try again tomorrow.")

    if os.path.exists(CSV_FILE_NAME):
        df_total = pd.read_csv(CSV_FILE_NAME)
        print(f"TOTAL PROGRESS: You now have {len(df_total)} videos collected in '{CSV_FILE_NAME}'.")

from google.colab import files
files.download("youtube_tech_data_final.csv")

